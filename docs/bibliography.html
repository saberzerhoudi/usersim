<!DOCTYPE html><!--JPbgWugsv3bv7hjUqd5Ql--><html lang="en" class="no-mobile no-touch "><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/usersim/_next/static/css/2f1550876c0c97ad.css" data-precedence="next"/><link rel="stylesheet" href="/usersim/_next/static/css/3879c13029a396ed.css" data-precedence="next"/><link rel="stylesheet" href="/usersim/_next/static/css/af5b0988c5924b30.css" data-precedence="next"/><link rel="stylesheet" href="/usersim/_next/static/css/33196766d86fd329.css" data-precedence="next"/><link rel="stylesheet" href="/usersim/_next/static/css/ea2e4ab832f89148.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/usersim/_next/static/chunks/webpack-8cb9ea34c04e71c3.js"/><script src="/usersim/_next/static/chunks/4bd1b696-9ad36e8faf1af39d.js" async=""></script><script src="/usersim/_next/static/chunks/255-84de894764491a04.js" async=""></script><script src="/usersim/_next/static/chunks/main-app-852d20233dc9b01e.js" async=""></script><script src="/usersim/_next/static/chunks/app/layout-ddaa13db89090814.js" async=""></script><script src="/usersim/_next/static/chunks/176-f06c1286f41227c9.js" async=""></script><script src="/usersim/_next/static/chunks/app/not-found-e6493035fdbf2ff2.js" async=""></script><script src="/usersim/_next/static/chunks/265-55e2ee7123c9abed.js" async=""></script><script src="/usersim/_next/static/chunks/app/bibliography/page-18a0c9430bc12292.js" async=""></script><link rel="icon" href="/usersim/icon.ico?ece3ef1deab37efa" type="image/x-icon" sizes="16x16"/><link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;700&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,500;1,400;1,500&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=Epilogue:wght@400;500&amp;family=Poppins&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:ital,wght@0,400;0,500;0,600;1,400&amp;display=swap" rel="stylesheet"/><link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:ital,wght@0,300;0,400;0,500;0,600;1,400&amp;display=swap" rel="stylesheet"/><script src="/usersim/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="appear-animate body"><div hidden=""><!--$--><!--/$--></div><div class="theme-main"><div class="page" id="top"><nav class="main-nav transparent stick-fixed wow-menubar"><div class="main-nav-sub full-wrapper"><div class="nav-logo-wrap local-scroll"><a class="logo" style="padding-left:30px" href="/usersim"><h2 style="font-weight:400;font-size:1.8rem;margin-top:15px"><span style="font-weight:900">User</span>Sim</h2></a></div><div class="mobile-nav" role="button" tabindex="0"><i class="mobile-nav-icon"></i><span class="visually-hidden">Menu</span></div><div class="inner-nav desktop-nav"><ul class="clearlist scroll-nav local-scroll scrollspyLinks"><li><a class="" href="/usersim">Home</a></li><li><a class="" href="/usersim/about">About</a></li><li class=""><a href="#" class="mn-has-sub">Resources<!-- --> <i class="mi-chevron-down"></i></a><ul class="mn-sub to-left "><li><a href="/usersim/toolkits">Toolkits</a></li><li><a href="/usersim/bibliography">Bibliography</a></li><li><a href="/usersim/tutorials">Tutorials</a></li></ul></li><li><a class="" href="/usersim/contributors">Contributors</a></li><li><a class="" href="/usersim/events">Events</a></li></ul></div></div></nav><main id="main"><section class="page-section pt-0 pb-0" id="home"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></section><section class="page-section pt-0" id="bibliography"><div class="container position-relative"><div class="row mb-60"><div class="col-lg-10 offset-lg-1"><div class="row"><div class="col-md-6 mb-20 mb-md-0"><div class="form-group"><input type="text" class="input-lg round form-control" placeholder="Search papers by title, authors, or keywords..." style="padding:12px 20px;font-size:15px" value=""/></div></div><div class="col-md-3 mb-20 mb-md-0"><select class="input-lg round form-control" style="padding:12px 20px;font-size:15px"><option value="all" selected="">All Years</option><option value="2024">2024</option><option value="2023">2023</option><option value="2022">2022</option></select></div><div class="col-md-3"><select class="input-lg round form-control" style="padding:12px 20px;font-size:15px"><option value="all" selected="">All Venues</option><option value="ACL">ACL</option><option value="CIKM">CIKM</option><option value="CUI">CUI</option><option value="EMNLP">EMNLP</option><option value="ICTIR">ICTIR</option><option value="NAACL">NAACL</option><option value="SIGIR">SIGIR</option><option value="UM-CIR">UM-CIR</option><option value="WSDM">WSDM</option><option value="arXiv">arXiv</option></select></div></div><div class="mt-20 text-center text-gray small">Showing <!-- -->18<!-- --> of <!-- -->18<!-- --> papers</div></div></div><div class="row mt-n30"><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">WSDM<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Abbasiantaeb et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2312.02913" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2312.02913" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a simulation framework involving two large language models (LLMs) acting as a questioner and an answerer engaged in a conversation. The main objectives are to investigate the effectiveness of LLMs in simulating question-answering conversations and to compare the generated conversations against human-human conversations with regards to various characteristics. The analysis shows that the LLMs tend to generate longer questions and answers than humans, and these provide better coverage of the topic in focus.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">CUI<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Bernard and Balog</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2405.14249" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2405.14249" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Identifying Breakdowns in Conversational Recommender Systems using User Simulation</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a methodology to systematically test conversational recommender systems with regards to conversational breakdowns. It consists of analyzing conversations generated between the system and a user simulator to identify pre-defined types of breakdowns. A case study demonstrates that the methodology can be applied to make an existing conversational recommender system more robust to conversation breakdowns.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">ICTIR<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Bernard and Balog</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2406.19007" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2406.19007" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Towards a Formal Characterization of User Simulation Objectives in Conversational Information Access</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Formally characterizes the distinct objectives of user simulators: (1) training aims to maximize behavioral similarity to real users and (2) evaluation focuses on the accurate prediction of real-world conversational agent performance. An empirical study shows that optimizing for one objective does not necessarily lead to improved performance on the other. This finding highlights the need for distinct design considerations during the development of user simulators.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">UM-CIR<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Fu et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://ceur-ws.org/Vol-3854/um-cir-1.pdf" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://ceur-ws.org/Vol-3854/um-cir-1.pdf" target="_blank" rel="noopener noreferrer" style="text-decoration:none">An Evaluation Framework for Conversational Information Retrieval Using User Simulation</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a new user simulator prototype to perform simulation-based evaluation of CIR systems. The user simulator comprises two modules: (1) action predictor and (2) response generator. The action predictor determines the next action based on the context, the actions available depends on the dataset used. The response generator uses the conversational context, user profile, and previously predicted action to output a realistic and personalised response. The assessment of success is based on the study of stopping strategies.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Huang et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2404.03304" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2404.03304" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Concept–An Evaluation Protocol on Conversation Recommender Systems with System-and User-centric Factors</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a new evaluation protocol for conversational recommendation systems that considers both system- and user-centric factors that influence the user experience and engagement. The protocol identifies and defines six abilities that relate to three factors, in addition to corresponding metrics. Some metrics are computed based on scores given by a large language model. The authors apply the protocol to evaluate off-the-self conversational recommender systems and demonstrate its comprehensiveness.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Vlachou and Macdonald</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2401.05783" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2401.05783" target="_blank" rel="noopener noreferrer" style="text-decoration:none">What Else Would I Like? A User Simulator using Alternatives for Improved Evaluation of Fashion Conversational Recommendation Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a meta user simulator that can provide knowledge on alternative targets in the context of conversational recommendations in fashion. Based on a patience parameter, the target item is replaced by the closest alternative (i.e., the one with the highest visual similarity). The experiments show that it leads to shorter conversations as users are inclined to change their minds and accept an alternative target; another positive consequence is an improved success rate of the conversational recommender system.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">NAACL<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Yoon et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2403.09738" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2403.09738" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a new protocol to evaluate LLM-based user simulators for conversational recommendation scenario. The protocol comprises five evaluation tasks: choosing items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendation, and giving feedback. The objective of these tasks is to discover distorsion between simulators and human behaviors. The experiments show that LLM-based simulators exhibit differences with humans such as low diversity in items discussed, low correlation with the representation/expression of preferences, a lack of personalization, and incoherent feedback.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Zhu et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2403.16416" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2403.16416" target="_blank" rel="noopener noreferrer" style="text-decoration:none">How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Performs an analysis of the limitations of a LLM-based user simulator, iEvaLM, and proposes a new user simulator to mitigate the identified limitations. The analysis of iEvaLM with two datasets reveals that (1) it leads to data leakage which inflate the performances, (2) the simulated responses are not the main factor in getting successful recommendations, and (3) controlling the simulated responses via a single prompt is complex. To address these limitations, the authors propose SimpleUserSim which does not know the name of the target item during the conversation and uses a different prompt for each possible user action. Using the same experimental setting, they show that SimpleUserSim is less prone to data leakage and produces more impactful responses.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2024</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Zhu et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2405.08035" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2405.08035" target="_blank" rel="noopener noreferrer" style="text-decoration:none">A LLM-based Controllable, Scalable, Human-Involved User Simulator Framework for Conversational Recommender Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Introduces a LLM-based user simulator that is controllable, scalable, and allows human intervention during user profile creation. The dialogue generation process is divided into stages: profile initialization, preference initialization, and message handling. The user behavior for each stage is controlled with specific plugins such as user preferences summary and intent understanding. These plugins can easily be modified, extended, or replaced to change the behavior of the user simulator. The authors perform experiments in two scenarios, with/out human annotation in the dataset, to showcase the adaptability of the user simulator and its ability to effectively simulate user preferences.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Davidson et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2309.13233" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2309.13233" target="_blank" rel="noopener noreferrer" style="text-decoration:none">User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a user simulator built with an LLM using in-context learning instead of fine-tuning. The main objective is to generate linguistically diverse and human-like utterances. The use of goal success rate as a metric to evaluate user simulator is criticized as humans tend to have non-optimal behavior.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">CIKM<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Hu et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2306.09821" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2306.09821" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a new optimization approach that leverages simulated user satisfaction from a large language model to enhance task-oriented dialogue systems. It integrated simulated user satisfaction into the reward function of proximal policy optimization used to optimize a fine-tuned task-oriented dialogue system. Empirical experiments with fine-tuned Flan-T5 (dialogue system) and ChatGPT (user simulator) on two benchmark datasets show the potential of the proposed approach in a case where user satisfaction annotations are not available.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">ACL<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Liu et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://aclanthology.org/2023.acl-long.1/" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://aclanthology.org/2023.acl-long.1/" target="_blank" rel="noopener noreferrer" style="text-decoration:none">One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes to train task-oriented dialogue systems using multiple user simulators. They define the problem as a multi-armed bandit where each arm corresponds to one user simulator. It allows to balance how much each simulator is used during optimisation steps and tackle catastrophic forgetting. The experimental results show that the performances are improved compared to baseline agents trained with a single user simulator in a single domain scenario; the agents trained with their framework are more robust to unseen domains. While the results are promising, they admit that experiments in a multi-domain scenario are needed.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">SIGIR<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Owoicho et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2304.13874" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2304.13874" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a user simulator able to answer clarifying questions and give direct feedback to a conversational search system. This simulator is initialized with a given information need and can interact with conversational search systems over multiple turns while using natural language and staying coherent. It also integrates the notion of patience, and will stop a conversation when patience runs out. Crowd workers assess the quality of the generated answers by the simulator w.r.t. naturalness and usefulness when used to perform experiments with the TREC CAsT dataset. The experiments show the benefits of using simulated user feedback to improve conversational search systems.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Sun et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2204.00763" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2204.00763" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Presents a metaphorical user simulator, MetaSim, that uses historical conversation strategies as metaphors for a current conversation. It improves the simulator&#x27;s abilities at dialogue reasoning and generalizing to new domains, and its realism. The authors also propose a tester-based evaluation framework to evaluate user simulators and task-oriented dialogue systems; a manual evaluation shows that it is a promising solution for automatic evaluation.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Terragni et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2306.00774" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2306.00774" target="_blank" rel="noopener noreferrer" style="text-decoration:none">In-Context Learning User Simulators for Task-Oriented Dialog Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes an approach to build an in-context learning user simulator using a LLM. The user simulator is given a prompt comprising the task description, example dialogues, user goal, and dialogue history to generate responses. It comprises an evaluation component that tracks the goal completion and assesses the system&#x27;s actions. The experiments show that the in-context learning abilities of LLMs are valuable to generate diverse dialogues (exploration of many dialogue paths) but suffer from limitations like unpredictability and hallucinations.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">arXiv<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Wang et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2306.02552" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2306.02552" target="_blank" rel="noopener noreferrer" style="text-decoration:none">User Behavior Simulation with Large Language Model based Agents</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Introduces a simulation environment where agents can interact with a recommender system, other agents, and &quot;social media&quot;. An agent is based on a LLM (ChatGPT in particular) and comprises three modules: profile, memory (inspired from cognitive neuroscience), and action. Two main questions need to be considered when leveraging LLMs: (1) what behavior to simulate and (2) how to design prompts.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">EMNLP<!-- --> <!-- -->2023</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Wang et al.</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://arxiv.org/abs/2305.13112" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://arxiv.org/abs/2305.13112" target="_blank" rel="noopener noreferrer" style="text-decoration:none">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes an interactive evaluation approach, iEvaLM, using LLM-based user simulators. The approach is validated through experimentation on two public datasets (ReDial and OpenDialKG). The simulated user is given a persona based on preferences established from ground truth items, and the allowed behaviors, i.e., taking about a preference, providing feedback, and completing the conversation, are defined in the prompt. The evaluation considers two types of metric that are objective with recall and subjective with persuasiveness (which is scored using an LLM). The authors also mention some of the limitations of this approach, mostly related to the LLM.</p></div></div></div></div></div></div><div class="col-12 mt-20"><div class="alt-features-item box-shadow p-30 p-sm-20"><div class="row"><div class="col-lg-3 mb-20 mb-lg-0 border-end"><div class="pe-lg-3"><span class="badge bg-dark text-white fw-bold mt-10 mb-20 px-3 py-2 rounded-pill">SIGIR<!-- --> <!-- -->2022</span><div class="mt-20 mb-30"><h6 class="small text-uppercase text-gray mb-10">Authors</h6><div class="small text-dark">Kim and Lipani</div></div><div class="mb-20"><h6 class="small text-uppercase text-gray mb-5">Access</h6><div class="link-hover-anim-underline small"><div class="mb-n10"><a href="https://discovery.ucl.ac.uk/id/eprint/10147689/" target="_blank" rel="noopener noreferrer" class="link-strong link-strong-unhovered"><i class="mi-link me-1"></i>View Paper</a></div><div><a class="link-strong link-strong-unhovered" style="cursor:pointer"><i class="mi-copy me-1"></i>Cite (BibTeX)</a></div></div></div></div></div><div class="col-lg-9"><div class="ps-lg-4"><h4 class="h4 mb-20 latex-font"><a href="https://discovery.ucl.ac.uk/id/eprint/10147689/" target="_blank" rel="noopener noreferrer" style="text-decoration:none">A Multi-Task Based Neural Model to Simulate Users in Goal Oriented Dialogue Systems</a></h4><div class="text-gray"><h6 class="small text-uppercase text-gray mb-5">Summary</h6><p class="mb-0 latex-font">Proposes a user simulator, based on a generative model, that predicts users&#x27; satisfaction scores, actions, and utterances in a multi-task learning setting. The authors perform an ablation study to show that the three tasks help each other to better simulate users. Note that the proposed user simulator does not represent users&#x27; knowledge and mental status.</p></div></div></div></div></div></div></div><hr class="mt-80 mb-60"/><div class="row"><div class="col-lg-10 offset-lg-1"><div class="text-center"><p class="text-gray mb-20">This page is maintained by<!-- --> <a href="mailto:nolwenn.bernard@th-koeln.de" class="link-hover-anim underline"><strong>Nolwenn Bernard</strong></a> <!-- -->and<!-- --> <a href="mailto:krisztian.balog@uis.no" class="link-hover-anim underline"><strong>Krisztian Balog</strong></a>.</p><p class="text-gray">We welcome suggestions via email.</p></div></div></div></div></section></main><footer class="page-section footer bg-gray-light-1 pt-60 pb-30"><div class="container"><div class="row text-gray"><div class="col-md-4 col-lg-6 offset-lg-1"><b>© Information Access &amp; Interaction research (IAI) <!-- -->2025<!-- -->.</b></div><div class="col-md-7 col-lg-3 offset-md-1 offset-lg-1 clearfix text-end"><b>Based in Stavanger, Norway.</b></div></div></div></footer></div></div><!--$--><!--/$--><script src="/usersim/_next/static/chunks/webpack-8cb9ea34c04e71c3.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7989,[],\"ClientSegmentRoot\"]\n3:I[9559,[\"177\",\"static/chunks/app/layout-ddaa13db89090814.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\n6:I[6660,[\"176\",\"static/chunks/176-f06c1286f41227c9.js\",\"345\",\"static/chunks/app/not-found-e6493035fdbf2ff2.js\"],\"default\"]\n8:I[1959,[],\"ClientPageRoot\"]\n9:I[3507,[\"176\",\"static/chunks/176-f06c1286f41227c9.js\",\"265\",\"static/chunks/265-55e2ee7123c9abed.js\",\"383\",\"static/chunks/app/bibliography/page-18a0c9430bc12292.js\"],\"default\"]\nc:I[4431,[],\"OutletBoundary\"]\ne:I[5278,[],\"AsyncMetadataOutlet\"]\n10:I[4431,[],\"ViewportBoundary\"]\n12:I[4431,[],\"MetadataBoundary\"]\n13:\"$Sreact.suspense\"\n15:I[7150,[],\"\"]\n:HL[\"/usersim/_next/static/css/2f1550876c0c97ad.css\",\"style\"]\n:HL[\"/usersim/_next/static/css/3879c13029a396ed.css\",\"style\"]\n:HL[\"/usersim/_next/static/css/af5b0988c5924b30.css\",\"style\"]\n:HL[\"/usersim/_next/static/css/33196766d86fd329.css\",\"style\"]\n:HL[\"/usersim/_next/static/css/ea2e4ab832f89148.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"JPbgWugsv3bv7hjUqd5Ql\",\"p\":\"/usersim\",\"c\":[\"\",\"bibliography\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"bibliography\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/usersim/_next/static/css/2f1550876c0c97ad.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/usersim/_next/static/css/3879c13029a396ed.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/usersim/_next/static/css/af5b0988c5924b30.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/usersim/_next/static/css/33196766d86fd329.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"/usersim/_next/static/css/ea2e4ab832f89148.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L2\",null,{\"Component\":\"$3\",\"slots\":{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"$L6\",null,{}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]},\"params\":{},\"promise\":\"$@7\"}]]}],{\"children\":[\"bibliography\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L8\",null,{\"Component\":\"$9\",\"searchParams\":{},\"params\":\"$0:f:0:1:1:props:children:1:props:params\",\"promises\":[\"$@a\",\"$@b\"]}],null,[\"$\",\"$Lc\",null,{\"children\":[\"$Ld\",[\"$\",\"$Le\",null,{\"promise\":\"$@f\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],null],[\"$\",\"$L12\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$13\",null,{\"fallback\":null,\"children\":\"$L14\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$15\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"7:\"$0:f:0:1:1:props:children:1:props:params\"\na:{}\nb:\"$0:f:0:1:1:props:children:1:props:params\"\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nd:null\n"])</script><script>self.__next_f.push([1,"16:I[622,[],\"IconMark\"]\nf:{\"metadata\":[[\"$\",\"link\",\"0\",{\"rel\":\"icon\",\"href\":\"/usersim/icon.ico?ece3ef1deab37efa\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"$L16\",\"1\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"14:\"$f:metadata\"\n"])</script></body></html>